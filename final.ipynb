{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as patches\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected = True) \n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 1000)\n",
    "from bokeh.models import Panel, Tabs\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "#import lightgbm as lgb\n",
    "import plotly.figure_factory as ff\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers \n",
    "from keras.layers import Reshape,Concatenate \n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "#from tensorboardcolab import *\n",
    "from tensorflow.keras.regularizers import l2  \n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Masking\n",
    "from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling2D\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec \n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "import tokenization\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Masking\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import fasttext\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling2D\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing import text, sequence \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.models import load_model\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras import backend\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 gh pg\n"
     ]
    }
   ],
   "source": [
    "l=[1,2,3,'gh','pg']\n",
    "print(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 13.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# question_text and document_text are the inputs.\n",
    "# bool_long means boolean value which indicates whether the data is short or long q&a.\n",
    "max_seq_length=200\n",
    "def predict(doc_text,ques_text,bool_long):\n",
    "    max_seq_length=200\n",
    "    \n",
    "    \n",
    "    \n",
    "## SHORT Q&A MODEL\n",
    "    if not bool_long:\n",
    "        bert_model = load_model(\"bertmodel\")\n",
    "        tokenizer = pickle.load(open(\"tokenizerf.pickel\",\"rb\")) \n",
    "\n",
    "# converting tokens to ID's\n",
    "        \n",
    "        \n",
    "        def TokenizeAndConvertToIds(text):\n",
    "            tokens= tokenizer.tokenize(text) # tokenize the reviews\n",
    "            tokens=tokens[0:(max_seq_length-2)] \n",
    "            tokens=['[CLS]',*tokens,'[SEP]'] # adding cls and sep at the end\n",
    "            masked_array=np.array([1]*len(tokens) + [0]* (max_seq_length-len(tokens))) # masking \n",
    "            segment_array=np.array([0]*max_seq_length) \n",
    "            if(len(tokens)<max_seq_length): \n",
    "                padding=['[PAD]']*(max_seq_length-len(tokens)) # padding\n",
    "                tokens=[*tokens,*padding] \n",
    "            tokentoid=np.array(tokenizer.convert_tokens_to_ids(tokens)) # converting the tokens to id\n",
    "            return tokentoid,masked_array,segment_array\n",
    "\n",
    "        #dtext=inputs_df['document_text']\n",
    "        #qtext=inputs_df['question_text']\n",
    "        \n",
    "        \n",
    "#token_to_id conversion of answer data      \n",
    "\n",
    "# dtext means document_text\n",
    "# qtext means question_text\n",
    " \n",
    "        dtext_tokens=[]\n",
    "        dtext_mask=[]\n",
    "        dtext_segment=[]\n",
    "\n",
    "        for i in doc_text: \n",
    "            tokentoid,masked_array,segment_array=TokenizeAndConvertToIds(i)\n",
    "            dtext_tokens.append(tokentoid)\n",
    "            dtext_mask.append(masked_array)\n",
    "            dtext_segment.append(segment_array)\n",
    "\n",
    "        \n",
    "        dtext_tokens=np.asarray(dtext_tokens)\n",
    "        dtext_mask=np.asarray(dtext_mask) \n",
    "        dtext_segment=np.asarray(dtext_segment) \n",
    "        \n",
    "# X_train_pooled_output of answers      \n",
    "        dtext_pooled_output=bert_model.predict([dtext_tokens,dtext_mask,dtext_segment])     \n",
    "        \n",
    "\n",
    "        tokenizer=pickle.load(open(\"tokenizerlong2f.pickel\",\"rb\"))\n",
    "        \n",
    "# token_to_sequences conversion of question data       \n",
    "        \n",
    "        def compute_text_and_questions1(ques_text,tokenizer): \n",
    "            question_tok = tokenizer.texts_to_sequences(ques_text)\n",
    "            question_tok = sequence.pad_sequences(question_tok)\n",
    "            return question_tok\n",
    "\n",
    "        question_tok=compute_text_and_questions1(ques_text,tokenizer)\n",
    "        \n",
    "# loading short model\n",
    "        \n",
    "        model1=load_model(\"shortmodelfinal\")\n",
    "        \n",
    "# Predicting the test data\n",
    "        dtext_temp=dtext_pooled_output.reshape(dtext_pooled_output.shape[0],dtext_pooled_output.shape[1],1) \n",
    "        predf1=model1.predict([question_tok,dtext_temp])    \n",
    "\n",
    "        \n",
    "        indf=[]\n",
    "        for i in range(len(predf1[0])): \n",
    "            arg_max=np.argmax(predf1[2][i])\n",
    "            indf.append(arg_max) \n",
    "            \n",
    "# Extracting YES,NO and NONE            \n",
    "        yesnolist=[]\n",
    "        for i in range(len(indf)):\n",
    "            if indf[i]==0:\n",
    "                yesnolist.append('NO')\n",
    "            elif indf[i]==1:\n",
    "                yesnolist.append('NONE')\n",
    "            elif indf[i]==2:\n",
    "                yesnolist.append('YES')\n",
    "\n",
    "# If NONE is the answer, then predict indices \n",
    "        l3=[]\n",
    "        if yesnolist[0]=='NONE':\n",
    "\n",
    "            for i in range(len(ques_text)):\n",
    "                j=''\n",
    "                print(\"Predicted Start index is: \",int(predf1[0][i][0]))\n",
    "                print(\"Predicted End index is: \",int(predf1[1][i][0]))        \n",
    "                print(\"Question is: \", ques_text[i])\n",
    "                for k in doc_text[i].split(' ')[int(predf1[0][i][0]):int(predf1[1][i][0])]:\n",
    "                    j=j+\" \"+k\n",
    "                print(\"Answer is: \",j)\n",
    "                print(\" \",end='\\n')\n",
    "        \n",
    "# YES or NO\n",
    "        else:\n",
    "            print(yesnolist[0])\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "## LONG Q&A MODEL                \n",
    "    else:\n",
    "        bert_model = load_model(\"bertmodel\")\n",
    "        tokenizer = pickle.load(open(\"tokenizerf.pickel\",\"rb\"))\n",
    "# converting tokens to ID's\n",
    "        \n",
    "        \n",
    "        def TokenizeAndConvertToIds(text):\n",
    "            tokens= tokenizer.tokenize(text) # tokenize the reviews\n",
    "            tokens=tokens[0:(max_seq_length-2)] \n",
    "            tokens=['[CLS]',*tokens,'[SEP]'] # adding cls and sep at the end\n",
    "            masked_array=np.array([1]*len(tokens) + [0]* (max_seq_length-len(tokens))) # masking \n",
    "            segment_array=np.array([0]*max_seq_length) \n",
    "            if(len(tokens)<max_seq_length): \n",
    "                padding=['[PAD]']*(max_seq_length-len(tokens)) # padding\n",
    "                tokens=[*tokens,*padding] \n",
    "            tokentoid=np.array(tokenizer.convert_tokens_to_ids(tokens)) # converting the tokens to id\n",
    "            return tokentoid,masked_array,segment_array\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "#token_to_id conversion of answer data      \n",
    "\n",
    "# dtext means document_text\n",
    " \n",
    "        dtext_tokens=[]\n",
    "        dtext_mask=[]\n",
    "        dtext_segment=[]\n",
    "\n",
    "        for i in doc_text: \n",
    "            tokentoid,masked_array,segment_array=TokenizeAndConvertToIds(i)\n",
    "            dtext_tokens.append(tokentoid)\n",
    "            dtext_mask.append(masked_array)\n",
    "            dtext_segment.append(segment_array)\n",
    "\n",
    "        dtext_tokens=np.asarray(dtext_tokens)\n",
    "        dtext_mask=np.asarray(dtext_mask) \n",
    "        dtext_segment=np.asarray(dtext_segment) \n",
    "        \n",
    "# X_train_pooled_output of answers      \n",
    "        dtext_pooled_output=bert_model.predict([dtext_tokens,dtext_mask,dtext_segment])     \n",
    "        \n",
    "\n",
    "        tokenizer=pickle.load(open(\"tokenizerlong2f.pickel\",\"rb\"))\n",
    "        \n",
    "# token_to_sequences conversion of question data       \n",
    "        \n",
    "        def compute_text_and_questions1(ques_text,tokenizer): \n",
    "            question_tok = tokenizer.texts_to_sequences(ques_text)\n",
    "            question_tok = sequence.pad_sequences(question_tok)\n",
    "            return question_tok\n",
    "\n",
    "        question_tok=compute_text_and_questions1(ques_text,tokenizer)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "# loading long model        \n",
    "        model1=load_model(\"longmodelfinal\")\n",
    "     \n",
    "        dtext_temp=dtext_pooled_output.reshape(dtext_pooled_output.shape[0],dtext_pooled_output.shape[1],1) \n",
    "#predicting the test data      \n",
    "        predf1=model1.predict([question_tok,dtext_temp])    \n",
    "      \n",
    "        print(' ',end='\\n')\n",
    "\n",
    "# questions and answers in the range of predicted indices\n",
    "        j=''\n",
    "        for i in range(len(doc_text)):\n",
    "            j=''\n",
    "            print(\"Predicted Start index is: \",int(predf1[0][i])) \n",
    "            print(\"Predicted End index is: \",int(predf1[1][i]))        \n",
    "            print(end='\\n')\n",
    "            print(\"Question is: \", ques_text[i])\n",
    "            for k in doc_text[i].split(' ')[int(predf1[0][i][0]):int(predf1[1][i][0])]:\n",
    "                j=j+\" \"+k\n",
    "            print(\"Answer is: \",j)\n",
    "            print(\" \",end='\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving input extracted from the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f83f2d3bf80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f83ddb273b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " \n",
      "Predicted Start index is:  1381\n",
      "Predicted End index is:  2339\n",
      "\n",
      "Question is:  which is the most common use of opt-in e-mail marketing\n",
      "Answer is:   than traditional mail , mainly because of the high cost and time required in a traditional mail campaign for producing the artwork , printing , addressing , and mailing . </Li> <Li> Businesses and organizations who send a high volume of emails can use an ESP ( email service provider ) to gather information about the behavior of the recipients . The insights provided by consumer response to email marketing help businesses and organizations understand and make use of consumer behavior . </Li> <Li> Email provides a cost - effective method to test different marketing content , including visual , creative , marketing copy , and multimedia assets . The data gathered by testing in the email channel can then be used across all channels of marketing campaigns , both print and digital . </Li> <Li> Advertisers can reach substantial numbers of email subscribers who have opted in ( i.e. , consented ) to receive the email . </Li> <Li> Almost half of American Internet users check or send email on a typical day , with emails delivered between 1 am and 5 am local time outperforming those sent at other times in open and click rates . </Li> <Li> Email is popular with digital marketers , rising an estimated 15 % in 2009 to £ 292 million in the UK . </Li> <Li> If compared to standard email , direct email marketing produces higher response rate and higher average order value for e-commerce businesses . </Li> </Ul> <H3> Disadvantages </H3> <P> As of mid-2016 email deliverability is still an issue for legitimate marketers . According to the report , legitimate email servers averaged a delivery rate of 73 % in the U.S. ; six percent were filtered as spam , and 22 % were missing . This lags behind other countries : Australia delivers at 90 % , Canada at 89 % , Britain at 88 % , France at 84 % , Germany at 80 % and Brazil at 79 % . </P> <P> Additionally , consumers receive on average circa 90 emails per day . </P> <P> Companies considering the use of an email marketing program must make sure that their program does not violate spam laws such as the United States ' Controlling the Assault of Non-Solicited Pornography and Marketing Act ( CAN - SPAM ) , the European Privacy and Electronic Communications Regulations 2003 , or their Internet service provider 's acceptable use policy . </P> <H2> Opt - in email advertising </H2> <P> Opt - in email advertising , or permission marketing , is a method of advertising via email whereby the recipient of the advertisement has consented to receive it . This method is one of several developed by marketers to eliminate the disadvantages of email marketing . </P> <P> Opt - in email marketing may evolve into a technology that uses a handshake protocol between the sender and receiver . This system is intended to eventually result in a high degree of satisfaction between consumers and marketers . If opt - in email advertising is used , the material that is emailed to consumers will be `` anticipated '' . It is assumed that the recipient wants to receive it , which makes it unlike unsolicited advertisements sent to the consumer . Ideally , opt - in email advertisements will be more personal and relevant to the consumer than untargeted advertisements . </P> <P> A common example of permission marketing is a newsletter sent to an advertising firm 's customers . Such newsletters inform customers of upcoming events or promotions , or new products . In this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . </P> <P> With a foundation of opted - in contact information stored in their database , marketers can send out promotional materials automatically using autoresponders -- known as drip marketing . They can also segment their promotions to specific market segments . </P> <H2> Legal requirements </H2> <H3> Australia </H3> <P> The Australian Spam Act 2003 is enforced by the Australian Communications and Media Authority , widely known as `` ACMA '' . The act defines the term unsolicited electronic messages , states how unsubscribe functions must work for commercial messages , and gives other key information . Fines range with 3 fines of AU $110,000 being issued to Virgin Blue Airlines ( 2011 ) , Tiger Airways Holdings Limited ( 2012 ) and Cellar master Wines Pty Limited ( 2013 ) . </P> <H3> Canada </H3> <P> The `` Canada Anti-Spam Law '' ( CASL ) went into effect on July 1 , 2014 . CASL requires an explicit or implicit opt - in from users , and the maximum fines for noncompliance are CA $ 1 million for individuals and $10 million for businesses . </P> <H3> European Union </H3> <P> In 2002 the European Union ( EU ) introduced the Directive on Privacy and Electronic Communications . Article 13 of the Directive prohibits the use of personal email addresses for marketing purposes . The Directive establishes the opt - in regime , where unsolicited emails may be sent only with prior agreement of the recipient ; this does not apply to business email addresses . </P> <P> The directive has since been incorporated into the laws of member states . In the UK it is covered under the Privacy and Electronic Communications ( EC Directive ) Regulations 2003 and applies to all organizations that send out marketing by some form of electronic communication . </P> <H3> United states </H3> <P> The CAN - SPAM Act of 2003 was\n",
      " \n",
      "Predicted Start index is:  197\n",
      "Predicted End index is:  374\n",
      "\n",
      "Question is:  how i.met your mother who is the mother\n",
      "Answer is:   by David Henrie ) </Td> </Tr> <Tr> <Th> Nationality </Th> <Td> American </Td> </Tr> </Table> <P> Tracy McConnell , better known as `` The Mother '' , is the title character from the CBS television sitcom How I Met Your Mother . The show , narrated by Future Ted , tells the story of how Ted Mosby met The Mother . Tracy McConnell appears in 8 episodes from `` Lucky Penny '' to `` The Time Travelers '' as an unseen character ; she was first seen fully in `` Something New '' and was promoted to a main character in season 9 . The Mother is played by Cristin Milioti . </P> <P> The story of how Ted met The Mother is the framing device behind the series ; many facts about her are revealed throughout the series , including the fact that Ted once unwittingly owned her umbrella before accidentally leaving it behind in her apartment . Ted and The Mother meet at the Farhampton train station following Barney Stinson and Robin Scherbatsky 's wedding\n",
      " \n",
      "CPU times: user 32.4 s, sys: 4.11 s, total: 36.5 s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#long answer\n",
    "predict(doc_text,ques_text,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f83cf341950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f83c141bd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Predicted Start index is:  466\n",
      "Predicted End index is:  476\n",
      "Question is:  which is the most common use of opt-in e-mail marketing\n",
      "Answer is:   hide ) <Ul> <Li> 1 History </Li> <Li> 2 Types\n",
      " \n",
      "Predicted Start index is:  412\n",
      "Predicted End index is:  420\n",
      "Question is:  how i.met your mother who is the mother\n",
      "Answer is:   from fans . </P> <P> An alternate ending\n",
      " \n",
      "CPU times: user 31.9 s, sys: 4 s, total: 35.9 s\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#short answer\n",
    "predict(doc_text,ques_text,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing input from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, sample = True, chunksize = 50000): \n",
    "    if sample == True:\n",
    "        df = [] \n",
    "        with open(path, 'rt') as reader:\n",
    "            for i in range(chunksize):\n",
    "                df.append(json.loads(reader.readline()))\n",
    "        df = pd.DataFrame(df)\n",
    "        print('Our sampled dataset have {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "    else:\n",
    "        df = pd.read_json(path, orient = 'records', lines = True)\n",
    "        print('Our dataset have {} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
    "        gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our sampled dataset have 50000 rows and 6 columns\n"
     ]
    }
   ],
   "source": [
    "train1 = read_data('simplified-nq-train.jsonl', sample = True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Email marketing - Wikipedia &lt;H1&gt; Email marketi...</td>\n",
       "      <td>which is the most common use of opt-in e-mail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Mother ( How I Met Your Mother ) - wikiped...</td>\n",
       "      <td>how i.met your mother who is the mother</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       document_text  \\\n",
       "0  Email marketing - Wikipedia <H1> Email marketi...   \n",
       "1  The Mother ( How I Met Your Mother ) - wikiped...   \n",
       "\n",
       "                                       question_text  \n",
       "0  which is the most common use of opt-in e-mail ...  \n",
       "1            how i.met your mother who is the mother  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te=pd.DataFrame(train1,columns=['document_text','question_text'])[:2]\n",
    "te.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "te=te.iloc[35] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_text    List of Italian musical terms used in English ...\n",
       "question_text              what is the musical term for fast tempo\n",
       "Name: 35, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_text=[te['document_text']]\n",
    "ques_text=[te['question_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text=[]\n",
    "for i in te['document_text'].values:\n",
    "    doc_text.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_text=[]\n",
    "for i in te['question_text'].values:\n",
    "    ques_text.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
